Query: How do I read or download web page content?
-----------------------------------------------------------------------

Rank:1 (https://stackoverflow.com/questions/7138397)
 You don't need servlet to read data from a remote server. You can just use http://download.oracle.com/javase/1.4.2/docs/api/java/net/URL.html#getContent%28%29 or <a href="http://download.oracle.com/javase/1,5.0/docs/api/java/net/URLConnection.html#getContent%28%29"Connection  class to read remote content from HTTP server. For example,  

  InputStream input = (InputStream) new URL("http://www.google.com").getContent();
  


-------------------------------------next answer-------------------------------------

Rank:2 (https://stackoverflow.com/questions/34312506)
 There are a number of mistakes your code: 

 
  You are reading into a character buffer with a fixed size.  
  You are ignoring the result of the  read(char[])  method.  It returns the number of characters actually read ... and you need to use that.  
  You are assuming that  read(char[])  will read all of the data.  In fact, it is only guaranteed to return at least one character ... or zero to indicate that you have reached the end of stream.  When you reach from a network connection, you are liable to only get the data that has already been sent by the other end and buffered locally.  
  When you create the String from the  char[]  you are assuming that every position in the character array contains a character from your stream.  
 

 There are multiple ways to do it correctly, and this is one way: 

  public String readIt(InputStream stream) throws IOException {
    Reader reader = new InputStreamReader(stream, "UTF-8");
    char[] buffer = new char[4096];
    StringBuilder builder = new StringBuilder();
    int len;
    while ((len = reader.read(buffer) > 0) {
        builder.append(buffer, 0, len);
    }
    return builder.toString();
}
  

 Another way to do it is to look for an existing 3rd-party library method with a  readFully(Reader)  method. 


-------------------------------------next answer-------------------------------------

Rank:3 (https://stackoverflow.com/questions/9826018)
 Use http://jsoup.org/. 

 You will be able to parse the content using css style selectors. 

 In this example you can try 

  Document doc = Jsoup.connect("http://www.uefa.com/uefa/aboutuefa/organisation/congress/news/newsid=1772321.html#uefa+moving+with+tide+history").get(); 
String textContents = doc.select(".newsText").first().text();
  


-------------------------------------next answer-------------------------------------

Rank:4 (https://stackoverflow.com/questions/7138410)
 What you are trying to do is called web scraping. Kayak and similar websites do it. Do search for it on web ;) Well in java you can do this. 

  URL url = new URL(<your URL>);

BufferedReader in = new BufferedReader(new InputStreamReader(url.openStream()));
String inputLine;
StringBuffer response = new StringBuffer();

while ((inputLine = in.readLine()) != null) {
  response.append(inputLine + "\n");
}

in.close();
  

 response will give you complete HTML content returned by that URL. 


-------------------------------------next answer-------------------------------------

Rank:5 (https://stackoverflow.com/questions/7128149)
 My advice is to use the http://jsoup.org/ library 

 It's very easy to parse an external content with a css/jquery-like syntax 

  // Only one line to parse an external content
Document doc = Jsoup.connect("http://jsoup.org").get();

// "Javascript-like" syntax
Element content = doc.getElementById("content");
Elements links = content.getElementsByTag("a");
for (Element link : links) {
  String linkHref = link.attr("href");
  String linkText = link.text();
}

// "Jquery/Css-like" syntax
Elements resultLinks = doc.select("h3.r > a");
Elements pngs = doc.select("img[src$=.png]");
  

 Just add the jsoup.jar library to your classpath and enjoy ! 
Open-Source and free to use of course. 



