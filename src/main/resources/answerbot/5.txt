----------------- No.5 -----------------
query : fast processing of data

----Summary----
Honestly Reading That Much Data With Java Is Probably Going To Be A Nightmare. I Only Worked Up To 5 Gb Of Text Files And It Was Really Slow And Difficult. You Could Use Something Closer To The Os Sedgrepfindetc . If Java Is A Must Then Nio Packages I Suppose Will Be Faster Then Simple File.
I Would Use A Blockingqueue For That. Simple And Built Into Java . I Do Something Similar Using Realtime Data From Chicago Merchantile Exchange. The Data Is Sent To One Place For Realtime Use And To Another Place Via Tcp Using A Blockingqueue Producer/Consumer To Persist The Data To A Database Oracleh2 . The Consumer Uses A Time Delayed Commit To Avoid Fdisk Sync Issues In The Database . H2 Type Databases Are Asyncronous Commit By Default And Avoid That Issue I Log The Persisting In The Consumer To Keep Track Of The Queue Size To Be Sure It Is Able To Keep Up With The Producer. Works Pretty Good For Me.
Would A Tuple Space / Javaspace Work? Also Check Out Other Enterprise Data Fabrics Like Oracle Coherence And Gemstone .
For Low Latency You Can Only Rely On Inmemory Data Access Disks Are Physically Too Slow And Ssds Too . If Data Does Not Fit In The Memory Of A Single Machine We Have To Distribute Our Data To More Nodes Summing Up Enough Memory.
For Persistency We Have To Write Our Data To Disk After All. Supposing Optimal Organization This Can Be Done As Background Activity Not Affecting Latency. However For Reliability Failover Ha Or Whatever Disk Operations Can Not Be Totally Independent Of The Access Methods We Have To Wait For The Disks When Modifying Data To Make Shure Our Operation Will Not Disappear. Concurrency Also Adds Some Complexity And Latency.
----------------------------------------