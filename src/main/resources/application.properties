debug=false
server.port= 8080
#server.port= 80

spring.jpa.database=POSTGRESQL
spring.datasource.platform=postgres
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.database.driverClassName=org.postgresql.Driver
spring.devtools.livereload.enabled = false

endpoints.cors.allowed-origins=*
spring.jpa.show-sql=false
#logging.level.org.springframework.web=DEBUG
#logging.level.org.hibernate.SQL=DEBUG
#logging.file = coderec.log
#logging.config = classpath:logback.xml


############################## INPUT PARAMETERS ######################################

spring.datasource.url=jdbc:postgresql://localhost:5432/stackoverflow2018java
spring.datasource.username=postgres
spring.datasource.password=lascampostgres5k

# This file must exists anywhere the app runs
pathFileEnvFlag = /home/rodrigo/tmp/environmentFlag.properties
#useProxy=true|false
#tagFilter = java    --run script to fill postsmin table containing only java posts 


 
#Phase 1: Select 1/3 of the external questions (without RACK). Run steps 2 to 9 for each question using features default weights (all set to 0.5) and adjuster default weights (all set to 1.0) and store posts according to their ranking order. Also, after step 6, store all related posts for each question.
     #11: first lot of 11 questions of each site
     #12: second lot of 11 questions of each site 
     #13: adjustments and fixes
#Phase 2: Run internal survey for the selected questions in Phase 1 to evaluate answers. For each question, retrieve the ranked posts previously stored. Store the evaluation of each user.
	 #11: first lot of 11 questions of each site
     #12: second lot of 11 questions of each site
#Phase 3: 
     #3.1: Build excel with evaluations. Manually open spreadsheet with libreoffice and follow the steps as in instructionsToAgreementPhaseLinks.txt . Agreement phase. 
     #3.2: Use previous xlsx spreedShed to build a matrix of values for the scales - before agreement
     #3.3: Use previous xlsx spreedShed to build a matrix of values for the scales - after agreement
     #3.4: Discover the best adjuster weights. Use the previous xlsx to load the scales and discover what is the influence of the post origin to the post quality. Set adjuster weights.
#Phase 4: Select another 1/3 of the remaining questions and run the approach like in phase 1. - Using adjuster weights.
	 #41: first lot of 11 questions of each site
     #42: second lot of 11 questions of each site 
#Phase 5: Repeat phase 2 (internal survey) for the selected questions. 
#Phase 6: 
     #6.1: Build excel with evaluations. Manually open spreadsheet with libreoffice and follow the steps as in instructionsToAgreementPhaseLinks.txt . Agreement phase. 
     #6.2: Use previous xlsx spreedShed to build a matrix of values for the scales - before agreement
     #6.3: Use previous xlsx spreedShed to build a matrix of values for the scales - after agreement
     #6.4: Generate ndcg for before and after aplying adjuster weights. Conclude if adjuster weights helped to improve the accuracy.  
     #6.5: Discover the best weights for the composer. For each question, retrieve the previously stored related posts, bootstrap weights, run steps 7 to 9 using each set of weights, build a ranked list and generate recall-rates and mrr. In the end, store the k best results and their weights. Compare perspectives results.
     #6.6: Apply weights. Generade ndcg considering all questions. 
#Phase 7: Using the best set of weights for the composer and the best set of adjuster weights, run the approach for the remaining 1/3 of questions in two perspectives ( with and without rack). 
#Phase 8: Repeat the internal survey for the selected questions.
#Phase 9: Generate mrr for the the two perspectives. Discover which one has the best mrr. 
#Phase 10: For each question in the best evaluated perspective (with or without RACK), retrieve the related posts previously stored, run steps 7 to 9 using the best weights for the composer and store the ranked list. 
#Phase 11: Run external survey to evaluate answers. Load each question and its ranked list and store users evaluations. 
#Phase 12: Use users ratings to generate recall-rates and mrr.
phaseNumber=6

#only valid to some sections
#if phase = 1 or 2, section = 11 or 12. If phase = 4, section = 40
section=4

#possible values: [1,2,3]. Each value correspond to a range of external questions ids.
#1: 1-11; 34-44; 67-77
#2: 12:22; 45-55; 78-88
#3: 23-33; 56-66; 89-99
lot=1

#external question field 
obs = questions without rack
#blank to all
numberOfQueriesToTest =
shuffleListOfQueriesBeforeGoogleSearch = false

#if false, disable RACK tool
runRack = false
numberOfRackClasses = 2

#if false, use a static list
runGoogleSearch = true   
numberOfGoogleResults = 10
APIKEY				= AIzaSyDtJV3KNblIpJfWGyKqSLMXntcZZlEZGFw
CUSTOM_SEARCH_ID    = 001510221793838672117:ix_l9bnjhbq



#when performing stemming and removing stop words, prune words with minimum size equals or less than:
minTokenSize=0

#other important control variables

internalSurveyRankListSize=10
externalSurveyRankListSize=10
metricsGenerationRankListSize=15

#ranking weights
alphaCosSim  	  = 0.50
betaCoverageScore = 0.50
gamaCodeSizeScore = 0.50
deltaRepScore     = 0.50
epsilonUpScore    = 0.50

#relation type weights
#relationType_FROM_GOOGLE_QUESTION = 0.56
#relationType_RELATED_DUPE 		  = 0.43
#relationType_RELATED_NOT_DUPE     = 0.36
#relationType_FROM_GOOGLE_ANSWER   = 0.24
#relationType_LINKS_INSIDE_TEXTS   = 0.0
relationType_FROM_GOOGLE_QUESTION = 1
relationType_RELATED_DUPE 		  = 1
relationType_RELATED_NOT_DUPE     = 1
relationType_FROM_GOOGLE_ANSWER   = 1
relationType_LINKS_INSIDE_TEXTS   = 1

