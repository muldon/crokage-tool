#### app environment variables
useProxy=false
environment=development
virutalPythonEnv=
#if true, load vectors only once, not by demand 
iHaveALotOfMemory=true

######### Must be set

#Download Extended our version of BIKER into this folder
#Download from https://drive.google.com/drive/folders/1B1ZDJ0yDAFFqbtHl2u-6FeG3ggQptlOm?usp=sharing
BIKER_HOME=/home/rodrigo/projects/BIKER/StackOverflow
CROKAGE_HOME=/home/rodrigo/Dropbox/Doutorado/projects/bot
TMP_DIR=/home/rodrigo/tmp
FAST_TEXT_INSTALLATION_DIR=/home/rodrigo/projects/fastText-0.1.0

#Fixed value - Download NLP2API-Replication-Package project into this final folder - build this structure in your home
#Download from https://drive.google.com/drive/folders/1y-W0BllWlZwcRq2D0iEtP5qZ1nvJ_a2-
NLP2API_HOME=/home/rodrigo/projects/NLP2API-Replication-Package

################ API tools actions

## merge QuestionsToEvaluate.ods from both researchers. 
## save as xlsx manually
# buildMatrixForKappaBeforeAgreement      = use QuestionsToEvaluate.xlsx to build MatrixForKappa.csv
# buildFileForAgreementPhaseHighlightingDifferences = process QuestionsToEvaluate.xlsx and build QueriesAndAnswersAgreement.xlsx with XXX where likert difference is > 1
# buildMatrixForKappaAfterAgreement      = use QueriesAndAnswersAgreement.xlsx to build MatrixForKappa.csv 
# loadGroundTruthSelectedQueries         = load queries and evaluations considering only queries whose answers contain at least one 4 likert

# tester							     = used for tests  
# checkConditions						 = used to check conditions to run the approach
# crawlGoogleForRelatedQuestionsIdsDatasetNLP2Api = use Google to build top k(10) related questins Ids
# crawlGoogleForRelatedQuestionsIdsDatasetCrokage = use Google to build top k(10) related questins Ids
# crawlStackExchangeForRelatedQuestionsIdsDatasetNLP2Api = use StackExchange API to fetch (numberOfGoogleResults) related questins Ids
# crawlStackExchangeForRelatedQuestionsIdsDatasetCrokage
# crawlBingForRelatedQuestionsIdsDatasetNLP2Api 
# crawlBingForRelatedQuestionsIdsDatasetCrokage 
# crawlBingForRelatedQuestionsIdsDatasetBIKER  
# processBingResultsToStandardFormat      = read text file and generate a standard map file query-> ids
# processCrawledQuestionsIDsAndBuilExcelFileForEvaluation = read the results from Google to NLP2Api questions and build an excel file to be evaluated
# generateAnswersIdsParentsMap           = used for cache purposes
# readAnswersIdsParentsMap               = used for cache purposes
# generateQuestionsIdsTitlesMap           = used for cache purposes
# generateQuestionsIdsContentMap          = used for cache purposes
# readQuestionsIdsTitlesMap               = read generated map
# generateInputQueriesFromExcelGroudTruth = use an excel file to fetch the queries
# generateInputQueriesFromNLP2ApiGroudTruth = use gold set from NLP2Api to generate input queries file
# extractAPIsFromRACK             		  = extracts APIs from RACK approach
# extractAPIsFromBIKER             		  = extracts APIs from BIKER approach
# extractAPIsFromNLP2Api				  = extracts APIs from NLP2Api approach
# getApisForApproaches                    = extracts APIs from all approaches
# loadExcelGroundTruthQuestionsAndLikerts = load excel files containing likerts for external questions - ground truth
# generateMetricsForApiExtractors 		  = extract metrics for the apis. Set subAction to RACK|BIKER|NLP2Api or a combination of them like BIKER|NLP2Api for BICKER + NLP2API or RACK|BIKER|NLP2Api for the three of them (the order matters !) 
#                                           subAction may also contains ...|generateTableForApproaches to print a string that can be used to paste into overleaf representing the lines of a table
# generateMetricsForBaselineApproaches    = extract metrics for other approaches (Google, Bing, SO ) as baselines.   
# generateInvertedIndexFileFromSOPosts    = fetches posts containing code, extract classes and build a file with inverted indexes in a format like: api-> postID1, postID2 ...   
# generatePostsApisMap                    = fetches posts containing code, extract classes and build a file with a map of classes for each post like: postId-> api1, api2...
# generateGroundTruthThreads              = generate a file containing the ground truth threads ids for queries
# loadPostsApisMap
# reduceBigMapFileToMininumAPIsCount      = loads the big file, set cutoff to the minimum amount of ids each class should have and generate reduced map
# loadInvertedIndexFile					  = loads the reduced file containing the indexes and stores into a map	
## preprocess another project
# generateTrainingFileToFastText		  = generate a txt file containing all posts contents (pre processed)
## build fastText model .bin              = ./fasttext skipgram -input /home/rodrigo/tmp/soContent.txt -output /home/rodrigo/tmp/fastTextModel -epoch 10 -lr 0.05 -minn 2 -maxn 5 -dim 100
# buildSODirectoryFiles				      = build a txt file representing each post to be used by lucene
# buildLuceneIndex						  = build lucene index	
# buildIDFVocabulary					  = build IDFs for all words in vocabulary	
# readIDFVocabulary					  	  = read IDFs for all words in vocabulary from a file
# buildSOSetOfWords						  = build a txt file with the set of words from SO. For this, read the idf txt file.
## build fastText wordvec txt file        = built by command line. Use .bin model to build the txt file with words and their vectors.
# readSoContentWordVectors				  = read vectors of all words from txt file, previously generated by fastText in shell 
# buildRelevantThreadsContents			  = build a txt file with contents of each relevant question (upvoted with answers containing code)
# runApproach							  = use an input query file to recommend answers #best combination of tools: rack|BIKER|NLP2Api
# extractAnswers						  = get the recommended results and extract code sentences with explanations
#                                           subAction = saveCache|useRecommendedCache. saveCache will save in a file the recommended results for each query. useRecommendedCache will use these saved results instead of calling approach. Test in lowercase.                                               	
## vm atributes: -Xms512m -Xmx15000m  

action=buildFileForAgreementPhaseHighlightingDifferences
subAction=rack|biker|NLP2Api|generatetableforapproaches|savecache|userecommendedcache

# if output file for queries (queriesApisOutput.txt) has already been generated, use false to avoid calling approaches again. Use true if output file with queriesApisOutput.txt has not been generated yet.
callBIKERProcess=false
callNLP2ApiProcess=false
callRACKApiProcess=false

# values= crokage|nlp2api|selectedQueries  
dataSet=selectedQueries

#concerns to logging infos. When true, logs only main infos.
productionEnvironment=true

# Number of queries to consider, null to all
limitQueries=
obs = w kler - Testando recall. Calculo do apiscore igual map. Filtro por apis apos merge.
numberOfAPIClasses=20
#-1 to all
topk=10
#uses when action=reduceBigMapFileToMininumAPIsCount
cutoff=2
topApisScoredPairsPercent=100
topSimilarContentsAsymRelevanceNumber=100
bm25TopNResults=10000
numberOfComposedAnswers=5

#if use Google to replace some steps
useGoogleSearch = false   
# max number allowed for the google api is 10
numberOfGoogleResults = 20
APIKEY				= AIzaSyAKwSG8WR7pdnfVCosoXs21O45t096qtKo
CUSTOM_SEARCH_ID    = 001510221793838672117:ix_l9bnjhbq

######### Must to be provided or generated via method call

BIKER_RUNNER_PATH=biker_runner_external_queries.py
#generated model path
FAST_TEXT_MODEL_PATH = ${TMP_DIR}/fastTextModel.bin
# generated by method call - a map containing all apis and their answers ids
BIG_MAP_INVERTED_INDEX_APIS_FILE_PATH = ${TMP_DIR}/bigMap.txt
# generated by method call - a map containing apis with a minimum count number
REDUCED_MAP_INVERTED_INDEX_APIS_FILE_PATH = ${TMP_DIR}/reducedMapReport.csv
# generated by method call by the same method above - answer ids disconsidered because have no api calls
SO_UPVOTED_POSTS_WITH_CODE_APIS_FILE = ${TMP_DIR}/soUpvotedPostsWithCodeAPIsMap.txt
# generated by method call.
SO_ANSWERS_IDS_PARENT_IDS_MAP = ${TMP_DIR}/soAnswersIdsParentsIds.txt
# generated by method call. Contains ids and titles from SO questions.
SO_QUESTIONS_IDS_TITLES_MAP = ${TMP_DIR}/soQuestionsIdsTitles.txt
# generated by method call. Contains ids and titles from SO questions.
SO_RELEVANT_THREADS_CONTENTS_MAP = ${TMP_DIR}/soRelevantThreadsContents.txt
# generated by method call. Contains ids and titles from SO questions.
SO_ANSWERS_IDS_CONTENTS_PARENT_CONTENTS_MAP = ${TMP_DIR}/soAnswersIdsContentsParentContents.txt
# generated by method call. Contains titles, bodies and code of SO posts, one per line
SO_CONTENT_FILE = ${TMP_DIR}/soContent.txt
# generated by method call. From so content file, calculate the idf of all words and save in this file.
SO_IDF_VOCABULARY = ${TMP_DIR}/soIDFVocabulary.txt
# generated by method call. From the idf file, build a file containing the non-repeated words of SO. Used to build the vectors for all words.
SO_SET_OF_WORDS = ${TMP_DIR}/soSetOfWords.txt
# generated in shell through fastText commands. Uses the file above. Contains word vectors for each word of the titles vocabulary.
SO_CONTENT_WORD_VECTORS = ${TMP_DIR}/soContentWordVec.txt
# generated by method call. To be used by lucene to build idf voc.
SO_DIRECTORY_FILES =  ${TMP_DIR}/sodirectory
# generated by method call. To be used by lucene to build idf voc.
SO_DIRECTORY_INDEX =  ${TMP_DIR}/sodirindex
# generated by method call. Top k results for bing.
BING_TOP_RESULTS_FOR_NLP2API_RAW_JSON = ${CROKAGE_HOME}/data/bingNLP2ApiResults-all-questions-rawJSON.txt
BING_TOP_RESULTS_FOR_CROKAGE_RAW_JSON = ${CROKAGE_HOME}/data/bingCrokageResults-all-questions-rawJSON.txt
BING_TOP_RESULTS_FOR_BIKER_RAW_JSON = ${CROKAGE_HOME}/data/bingBikerResults-all-questions-rawJSON.txt
# generated by method call. Top k results
BING_TOP_RESULTS_FOR_NLP2API = ${CROKAGE_HOME}/data/bingNLP2ApiResults-all-questions.txt
BING_TOP_RESULTS_FOR_CROKAGE = ${CROKAGE_HOME}/data/bingCrokageResults-all-questions.txt
BING_TOP_RESULTS_FOR_BIKER = ${CROKAGE_HOME}/data/bingBikerResults-all-questions.txt
GOOGLE_TOP_RESULTS_FOR_NLP2API = ${CROKAGE_HOME}/data/googleNLP2ApiResults-all-questions.txt
GOOGLE_TOP_RESULTS_FOR_CROKAGE = ${CROKAGE_HOME}/data/googleCrokageResults-all-questions.txt
SE_TOP_RESULTS_FOR_NLP2API = ${CROKAGE_HOME}/data/stackExchangeNLP2ApiResults-all-questions.txt
SE_TOP_RESULTS_FOR_CROKAGE = ${CROKAGE_HOME}/data/stackExchangeCrokageResults-all-questions.txt

# questions not relevant enough for evaluation
ALL_QUESTIONS_EXCEPTIONS_FOR_NLP2API = ${CROKAGE_HOME}/data/allQuestionsExceptionsForNLP2Api.txt
# to evaluate questions to queries. Can be .csv or .ods
QUERIES_AND_SO_ANSWERS_TO_EVALUATE =${CROKAGE_HOME}/data/QuestionsToEvaluate

QUERIES_AND_SO_ANSWERS_AGREEMENT =${CROKAGE_HOME}/data/QueriesAndAnswersAgreement.xlsx

GROUND_TRUTH_THREADS_FOR_QUERIES =${CROKAGE_HOME}/data/groundTruthThreadsForQueries.txt

#Answers directory
ANSWERS_DIRECTORY = ${TMP_DIR}/answers

######### Must to be generated or provided for tests

 #for tests - static file - must be provided
NLP2API_GOLD_SET_FILE = ${CROKAGE_HOME}/data/nlp2ApiGoldSet.txt
INPUT_QUERIES_FILE_CROKAGE = ${CROKAGE_HOME}/data/inputQueriesCrokage-42-selected-results.txt
INPUT_QUERIES_FILE_NLP2API = ${CROKAGE_HOME}/data/inputQueriesNlp2Api.txt
INPUT_QUERIES_FILE_BIKER = ${CROKAGE_HOME}/data/inputQueriesBiker.txt
INPUT_QUERIES_FILE_SELECTED_QUERIES = ${CROKAGE_HOME}/data/selectedQueries.txt
INPUT_QUERIES_FILE_SELECTED_QUERIES_HAVING_GOLD_SET = ${CROKAGE_HOME}/data/selectedQueriesHavingGoldSet.txt

STOP_WORDS_FILE_PATH=${CROKAGE_HOME}/data/stanford_stop_words.txt

RECOMMENDED_ANSWERS_QUERIES_CACHE =${CROKAGE_HOME}/data/recommendedAnswersQueriesCache.txt

######### Automatically generated files

#generated to be read by BIKER
BIKER_INPUT_QUERIES_FILE = ${BIKER_HOME}/data/inputQueries
#generated by BIKER
BIKER_OUTPUT_QUERIES_FILE = ${BIKER_HOME}/data/queriesApisOutput.txt
#generated to call biker approach
BIKER_SCRIPT_FILE = /tmp/bikerCaller.sh
#generated to feed RACK jar
RACK_INPUT_QUERIES_FILE = ${CROKAGE_HOME}/data/rackApiQueriesInput.txt
#generated after calling NLP2Api jar
RACK_OUTPUT_QUERIES_FILE = ${CROKAGE_HOME}/data/rackApiQueriesOutput.txt
#generated to feed NLP2Api jar
NLP2API_INPUT_QUERIES_FILE = ${CROKAGE_HOME}/data/nlp2apiQueriesInput.txt
#generated after calling NLP2Api jar
NLP2API_OUTPUT_QUERIES_FILE = ${CROKAGE_HOME}/data/nlp2apiQueriesOutput.txt

MATRIX_KAPPA_BEFORE_AGREEMENT = ${CROKAGE_HOME}/data/MatrixForKappaBeforeAgreement.csv
MATRIX_KAPPA_AFTER_AGREEMENT = ${CROKAGE_HOME}/data/MatrixForKappaAfterAgreement.csv

############################## Database and Spring parameters ######################################

debug=false
server.port= 8080
#server.port= 80

spring.jpa.database=POSTGRESQL
spring.datasource.platform=postgres
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=none
spring.database.driverClassName=org.postgresql.Driver
spring.devtools.livereload.enabled = false

endpoints.cors.allowed-origins=*
spring.jpa.show-sql=false
#logging.level.org.springframework.web=DEBUG
#logging.level.org.hibernate.SQL=DEBUG
#logging.file = coderec.log
logging.config = classpath:logback.xml

spring.datasource.url=jdbc:postgresql://localhost:5432/stackoverflow2018msr2019java
spring.datasource.username=postgres
spring.datasource.password=lascampostgres5k